\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{extpfeil}

\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{pgfplots} % 引入pgfplots宏包
\pgfplotsset{compat=1.18} % 设置兼容性版本
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage[table,xcdraw]{xcolor}
\usepackage{url} %cite

\usepackage{tcolorbox}

\colorlet{LightGray}{White!90!Periwinkle}
\colorlet{LightOrange}{Orange!15}
\colorlet{LightGreen}{Green!15}
\colorlet{LightBlue}{blue!5}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}

\declaretheoremstyle[name=Theorem,]{thmsty}
\declaretheorem[style=thmsty,numberwithin=section]{theorem}
\tcolorboxenvironment{theorem}{colback=LightGray}

\declaretheoremstyle[name=Definition,]{define}
\declaretheorem[style=define,numberwithin=section]{definition}
\tcolorboxenvironment{definition}{colback=LightOrange}

\declaretheoremstyle[name=Axiom,]{axiomsty}
\declaretheorem[style=axiomsty,numberwithin=section]{axiom}
\tcolorboxenvironment{axiom}{colback=LightGreen}

\declaretheoremstyle[name=Quiz,]{quizty}
\declaretheorem[style=quizty,numberwithin=section]{quiz}
\tcolorboxenvironment{quiz}{colback= LightBlue}




\setstretch{1.2}
\geometry{
    textheight=9in,
    textwidth=5.5in,
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}

% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
% ------------------------------------------------------------------------------

\title{ \normalsize \textsc{}
		\\ [2.0cm]
		\HRule{1.5pt} \\
		\LARGE \textbf{\uppercase{Notebook for MA215 Probability}
		\HRule{2.0pt} \\ [0.6cm] \LARGE{Lecturer: Prof.Hong} \vspace*{10\baselineskip}}
		}
\date{}
\author{\textbf{Author} \\ 
		Hongli Ye \\
		Southern University of Science and Technology \\
		2024 Fall}

\maketitle
\newpage

\tableofcontents
\newpage

% ------------------------------------------------------------------------------

\section{Lecture 1 Basic of Probability 2024.09.12}
\begin{theorem}
    \textbf{Basic principle of counting}\\
    Suppose there are two experiments. Experiment 1 has $n$ results and experiment 2 has $m$ results.\\
    Then together there are $m \times n$ possible outcomes.
\end{theorem}
This basic theorem could be extended to many finite experiments by induction.


\begin{definition}
    \textbf{Permutation}\\
    Permutation means the different ordered arrangement of objects.
\end{definition}

\begin{theorem}
    Suppose we have $n$ objects. Then there are $n! = \Pi^n_{i=1}(i) = 1 \times 2 \times \dots \times n$ possible permutations.
\end{theorem}

\begin{theorem}
    There are $n$ objects, of which $n_1$ are alike, $n_2$ are alike,$\dots$,$n_r$ are alike.\\
    Then there are $\frac{n!}{n_1!\times n_2!\times \dots n_r!}$ possible outcomes.
\end{theorem}

\begin{definition}
    \textbf{Combination}\\
    Combination refers to selecting items from a set where order does not matter.
\end{definition}

\begin{theorem}
    If we choose $r$ objects from a total of $n$ differents objects at a time, then the \# possible combinations of $\binom{n}{r}$
\end{theorem}

\begin{theorem}
    \textbf{Binomal Theorem}\\
    For any positive integer $n \geq 1$
    $$ (x + y)^k = \Sigma^n_{k=0}(\binom{n}{k}x^ky^{n-k})$$
\end{theorem}
\begin{definition}
    \textbf{Induction}\\
    Mathematical Induction is a proof method for natural numbers, consisting of a base case and an inductive step to show a statement holds for all natural numbers.
\end{definition}
Mathematical Induction's basic step:
\begin{enumerate}
    \item Basic step: The case holds when $n = 1$
    \item Inductive step: Assume $n = k$ holds for some $k \geq 1$. Then $n = k+1$ holds.
\end{enumerate}

\begin{quiz}
    From 8 women and 6 men, a committee of 3 men and 3 women is to be formed. How many different committees?
    \begin{enumerate}
        \item 2 of the men refuse to serve together?
        \item 2 of the women refuse to serve together?
        \item 1 man and 1 woman refuse to serve together?
    \end{enumerate}
\end{quiz}

% Maybe I need to add one more part: Examples.
% Set style and colour later.


\section{Lecture 2 Probability Space 2024.09.19}
\textbf{Probability Space} includes Sample Space, Events and Probability Measure.\\
Probability Space is a special case of measure theory.
\begin{definition}
    \textbf{Sample Space}\\
    The sample space $S$ is the set of all possible outcomes of an experiment.
\end{definition}

\begin{definition}
    \textbf{Event}\\
    An event is a subset of the sample space $S$, denoted $E \subset S$
\end{definition}

\begin{definition}
    \textbf{Set Operation}\\
    Let $E,F$ be two events and $S$ is the sample space.
    \begin{enumerate}
        \item \textbf{Union}: $ E \cup F = \{ x | x\in E \text{ or } x \in F\}$
        \item \textbf{Intersection}: $ E \cap F = \{ x | x\in E \text{ and } x \in F\}$
        \item \textbf{Complement}: $ E^c = \{ x | x \notin E \text{ and } x \in S\}$
        \item \textbf{Different}: $ E - F = \{ x | x\in E \text{ or } x \notin F\}$
    \end{enumerate}
\end{definition}

\begin{definition}
    \textbf{Extension:} $\sigma-\text{algebra}$\\
    Let $\mathcal{X}$ be a non-empty set. $\mathcal{F}$ is said to be a $\sigma$-algebra if:
    \begin{enumerate}
        \item $\mathbb{X} \in \mathbb{F}$
        \item If $A \in \mathcal{F}, A^c \in \mathcal{F}$
        \item If $A_1,A_2 \dots \in \mathcal{F}, \text{ then } \bigcup^{\infty}_{i=1}(A_i) \in \mathcal{F}$
    \end{enumerate}
\end{definition}

\begin{theorem}
    \textbf{De Morgan's Law}\\
    For each $n \geq 1$, we have
    $$ (\bigcup^n_{i=1}(E_i))^c = \bigcap^n_{i=1}(E^c_i)$$
    $$ (\bigcap^n_{i=1}(E_i))^c = \bigcup^n_{i=1}(E^c_i)$$
\end{theorem}

\begin{axiom}
    \textbf{Axiom of Probability}
    Let $S$ be a sample space. For each event $E$, the probability $P(E)$ satisfies:
    \begin{enumerate}
        \item $0 \leq P(E) \leq 1$
        \item $P(S) = 1$
        \item For any sequence of mutually exclusive events $E_1,E_2 \dots$, we have:
        $$ \Sigma^\infty_{i=1}P(E_i) = 1$$
    \end{enumerate}
\end{axiom}

\begin{theorem}
    \textbf{Basic corollaries:}\\
    \begin{enumerate}
        \item $P(E) = 1 - P(E^c)$
        \item If $ E \subset F$, then $ P(E) \leq P(F)$
        \item $P(E \cup F) = P(E) + P(F) - P(E \cap F)$
        \item Inclusion-Exclusion Identity:(Extension of the line above)
        $$ P(\bigcup^n_{i=1}) = \Sigma^n_{i=1}P(E_i) - \Sigma_{i_1 < i_2}P(E_{i_1} \cap E_{i_2}) + \dots + (-1)^{n+1}P(\bigcap^n_{i=1}(E_i))$$
    \end{enumerate}
\end{theorem}

\begin{quiz}
    There are $N$ cards numbered as $1,2,\dots, N$. Pick 1 card uniformly at random. Write down the number and return the card> Repeat for $n$ times ($n > N, n = N, n < N$), we get a sequence $(x_1,x_2\dots, x_n)$.
    \begin{enumerate}
        \item $P(\text{the sequence is strictly increasing})$
        \item $P(\text{the sequence is non-decreasing})$
    \end{enumerate}
\end{quiz}



\section{Lecture 3 Conditional Probability and Independence 2024.09.26}
\begin{definition}
    \textbf{Conditional Probability}\\
    For 2 events E,F such that $P(E) > 0$. The conditional probability F occurs given that E has occurred is denoted by:
    $$ P(F|E) = \frac{P(F \cap E)}{P(E)}$$
\end{definition}
This definition give a new perspective into the conditional probability:
\begin{theorem}
    If each outcome of a finite sample space is equally likely, then we may compute the conditional probability of the form $P(F|E)$ by using E as the reduced sample space.
\end{theorem}

\begin{theorem}
    \textbf{Multiplication Law}\\
    For events E, F, we have:
    $$ P(E \cap F) = P(E)\times P(F|E)$$
    More generally:
    $$ P(E_1 \cap E_2 \cap \dots \cap E_n) = P(E_1)\times P(E_2|E_1)\times P(E_3|(E_1 \cap E_2))\dots P(E_n|\bigcap^{n-1}_{i=1}(E_i))$$
\end{theorem}
Theorem 3.2 is also called \textbf{chain rule}, which can be used in induction or some other methods.
\begin{definition}
    \textbf{Independence}\\
    For two events E, F. We say E and F are independent if:
    $$ P(E \cap F) = P(E)\times P(F) \text{ or } P(F|E) = P(F)$$
\end{definition}

\begin{theorem}
    \textbf{Total Probability Formula}\\
    Let $A_1,A_2,\dots,A_n$ be mutually exclusive with $S = \bigcup^n_{k=1}(A_k)$.\\
    Then $\forall$ event B:
    $$ P(B) = \Sigma^n_{i=1}P(B|A_i)P(A_i)$$
\end{theorem}

\begin{theorem}
    \textbf{Bayes's Theorem}\\
    Let $A_1,A_2,\dots A_n$ be mutually exclusive so that $S = \bigcup^n_{k=1}(A_k)$.\\
    Then $\forall$ event B:
    $$ P(A_j|B) = \frac{P(B|A_j)\times P(A_j)}{\Sigma^n_{i=1}P(B|A_i)P(A_i)}$$
\end{theorem}

\begin{quiz}
    A gambler has a fair coin and a two-headed coin in his pocket.
    \begin{enumerate}
        \item He selects one of the coins at random; when he flips it, it shows heads. What is the probability that it is the fair coin?
        \item Suppose that he flips the same coin a second time and, again, it shows heads. Now what is the probability that it is fair coin?
        \item Suppose that he flips the same coin a third time and it shows tails. Now what is the probability that it is the fair coin?
    \end{enumerate}
\end{quiz}




\section{Lecture 4 Discrete Random Variable 2024.10.10}

\begin{definition}
    \textbf{Discrete Random Variable}\\
    A Random Variable $ X: S \longrightarrow \mathbb{R}$.\\
    If we take on at most a countable number of possible values is called discrete random R.V.
\end{definition}
For example: 80 students, for which 70 are male. Choose 1 uniformly at random. Do this for 4 times. Let $X = \#$ of male students chosen.

Then $X$ is a discrete R.V. taking values of \{0,1,2,3,4\}

Moreover:
$$\forall \text{ } k \in \{0,1,2,3,4\}. P(X=k) = \binom{4}{k}(\frac{7}{8})^k(\frac{1}{8})^{1-k}$$
This is the probability mass functor of $X$.
\begin{definition}
    \textbf{Probability Mass Functor}\\
    For a discrete random variable $X$, we can define the probability mass functor(p.m.f), where $p(m)$ of $X$ by
    $$ p(m) = P(X=m)$$
\end{definition}

\begin{definition}
    
    \textbf{Special Random Variable}
    \begin{enumerate}
        \item A random variable is said to be a \textbf{Bernoulli} random variable with parameter $p \in [0,1]$ if: 
        $$ P(X=0) = 1 - p \text{ and } P(X = 1) = p $$
        We say $ X \sim \text{Bernoulli}(p)$
        \item If we toss a coin independently for $n$ times and let $X = \#$ of heads coming up, then $X$ is said to be a \textbf{Binomial} random variable with parameter $p \in [0,1]$\\
        Denoted by $X \sim \text{Bin}(n,p)$
    \end{enumerate}
    
\end{definition}
The possible mass functor of $\text{Bin}(n,p)$ is:
$$ P(X=k) = \binom{n}{k}p^k(1-p)^{n-k}$$

For example: Alice is in a class of 80 students, after 100 independent trials. We count $X$ as the \# of times where Alice is picked. Then $X \sim \text{Bin}(100, \frac{1}{80})$


Remark: Binomial R.V. equals $n$ times the addition of Bernoulli R.V.


\begin{definition}
    \textbf{Poisson Random variable}\\
    Let $X = \text{Bin}(n,\frac{\lambda}{n})$ for some $\lambda > 0$.\\
    Then let $n \rightarrow \infty$, we can get a new p.m.f, which is the p.m.f of Poission R.V. :
    $$ P(X = k) = e^{-\lambda}\frac{\lambda^k}{k!} \textbf{ } \forall k \geq 0$$
    Denoted by $ X \sim \text{Poission}(\lambda)$
\end{definition}

\begin{definition}
    \textbf{Geometry Random Variable}\\
    There is a coin having probability $ p \in (0,1)$ of coming up heads. Toss the coin util it shows up head. Let $X = \#$ of tosses needed.\\
    Then $X \sim \text{Geometric}(p)$, then p.m.f. of which is :
    $$ P(X = k) = (1-p)^{k-1}p \text{ } \forall k \geq 1$$
    Denoted by $ X \sim \text{Geometric}(p)$
\end{definition}
The definition seems to be different from the Geometry Random Variable in Statistics. But they are actually the same.

\textbf{Coupon Collector Problem:}\\
Pick one card uniformly at random, record the number and then return the card. Repeat until we collect all the $n$ numbers.

What is the average number of trials needed?

\begin{definition}
    \textbf{Expectation}\\
    For a discrete random variable, the expectation of $X$ is defined by:
    $$ E(X) = \Sigma^n_{k=1}kP(X=k)$$
\end{definition}

\begin{quiz}
    Jim is conducting random walk on the real line starting from 0. For each time, independently of anything else, he moves one steps to the right with probability p, and to the left with probability $1-p$. Let $X_n$ be the position of Jim at time n. Find $P(X_n = k)$ for each $-n \leq k \leq n$
\end{quiz}




\section{Lecture 5 Continuous Random Variable 2024.10.12}
\begin{definition}
    \textbf{Probability Density Function}\\
    A non-negative function $f: (-\infty, \infty) \longrightarrow [0,\infty]$ is called a probability density function(p.d.f.), if:
    $$ \int^{\infty}_{-\infty} f(x) \, dx = 1$$
\end{definition}

\begin{definition}
    \textbf{Continuous Random Variable}\\
    A random variable $X$ is called a continuous random variable if exists a p.d.f. $f$ such that:
    $$\forall a,b \in \mathbb{R}, \text{ } P(a \leq X \leq b) = \int^b_a f(x) \, dx$$

\end{definition}
Remark: Let $b=a$ to get:
$$ P(X=a) = P(a\leq X\leq a) = \int^a_a f(x) \, dx = 0$$

\begin{definition}
    \textbf{Uniform Random Variable}\\
    A random variable $X \sim Uniform(\alpha,\beta)$, if the p.d.f. of $X$ is:
    \[
    f(x) = \frac{1}{\beta - \alpha} 1_{(\alpha, \beta)}(x) = 
    \begin{cases} 
    \frac{1}{\beta - \alpha} & \text{if } \alpha < x <\beta\\
    0 & \text{, otherwise}  
    \end{cases}
    \]
\end{definition}
\textbf{Indicator function:} 
\[
 1_A(x) =
 \begin{cases}
  1   &  x \in A \\
  0   &  x \notin A
 \end{cases}
\]
It is also called as characteristic function.\\
For example: Let $X \sim Unif(1,5)$, find $P(X > 3.5)$\\
Solution:
$$ P(X>3.5) = P(X \geq 3.5)$$
$$ P(X \geq 3.5) = lim_{b \rightarrow \infty}P(3.5 \leq X \leq b)$$
$$ P(X \geq 3.5) = \int^{\infty}_{3.5} f(x)\, dx = \frac{3}{8}$$

\begin{definition}
    \textbf{Exponential Randon Variable}\\
    We say a $X$ is an exponential random variable with parameter $\lambda > 0$ if the p.d.f. is:
    \[
    f(x) = 
    \begin{cases}
        \lambda e^{-\lambda x} & x \geq 0 \\
        0 & x < 0
    \end{cases}
    \]
\end{definition}

\begin{definition}
    \textbf{Memoryless}\\
    We say a random variable is memoryless if:
    $$ P(X > t+s | X>t) = P(X > s ) \text{ } \forall t,s>0$$
\end{definition}
It is easy to prove that all exponential random variables are memoryless.

\begin{theorem}
    If $X$ is memoryless, then $X \sim Exp(\lambda) \text{ for some } \lambda > 0$. \\
    Moreover, we proved that memoryless random variable equals to exponetial random variable.
\end{theorem}
For example: Let $X$ be a continuous random variable with p.d.f.
\[ 
f(x) =
\begin{cases}
    \lambda e^{-\lambda x} & x \geq 0 \\
    0 & x = 0
\end{cases}
\]
Calculate $P(50 \leq X \leq 150)$ \\
Solution:
\begin{enumerate}
    \item Use $\int^{\infty}_{\infty}f(x) \, dx = 1$, we can get $\lambda = \frac{1}{100}$
    \item $P(50 \leq X \leq 150) = P(X \geq 50) - P(X > 150) = e^{-\frac{50}{100} - e^{-\frac{150}{100}}}$
\end{enumerate}
\begin{definition}
    \textbf{Gamma function:}
    $$ \Gamma(\alpha) = \int^{\infty}_0 e^{-y}y^{\alpha-1}\, dy$$
    Moreover:
    $$ \text{If } \alpha = n \in \mathbb{N}, \text{ then } \Gamma(n) = (n-1)!$$
\end{definition}

\begin{definition}
    \textbf{Gamma Random Variable}\\
    Let $X$ be a Gamma Random Variable, denoted by $X \sim Gamma(n,\lambda)$,then its p.d.f. is:
    $$ f(x) = \frac{x^{n-1} e^{-x/\lambda}}{\lambda^n \Gamma(n)}, \quad x > 0$$
\end{definition}
In fact, if $X_1, X_2, \dots ,X_n$ are independent $Exp(\lambda)$, then
$$ X_1 + X_2 + \dots + X_n \sim \text{Gamma}(n, \lambda)$$
We can understand the Gamma random variable in both two ways.
\begin{definition}
    \textbf{Normal Random Variable}\\
    We say a $X \sim N(\mu, \sigma^2)$ is a normal random variable if the density is:
    $$ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \textbf{ } \forall -\infty < x < \infty$$
\end{definition}

\begin{definition}
    \textbf{Expectation}\\
    For a continuous random variable $X$, the expectation of $X$ is defined by
    $$EX = E(X) = E[X] = \int^{\infty}_{\infty} xf(x) \, dx$$
\end{definition}
For any function $g$, we have:
$$ E(g(X)) = \int^{\infty}_{-\infty} g(x)f(x) \, dx$$
Note: Expectation is actually a integration of a measurement.

\begin{theorem}
    \textbf{Properties of expectation}\\
    Let $X$,$Y$ be two random variables:
    \begin{enumerate}
        \item $\forall c \in \mathbb{R}, E(c) = c$
        \item If $X \geq 0$, then $EX \geq 0$
        \item If $c \in \mathbb{R}, E(cX) = cEX$
        \item $E[X+Y] = E[X] + E[Y]$
    \end{enumerate}
    By properties 3 and 4, we know that expectation is \textbf{linear}.
\end{theorem}
For example: $X \sim Unif(0,1)$
$$ EX = \int^{\infty}_{\infty} xf(x) \, dx = \frac{1}{2}$$
\begin{definition}
    \textbf{Variance}\\
    The variance of $X$ is given by:
    $$ Var(X) = E[(X - EX)^2]$$
    Moreover, we could also calculate by:
    $$ Var(X) = E(X^2) - (EX)^2$$
\end{definition}

\begin{quiz}
    Prove
    $$ \mu = \int^\infty_{-\infty}xf(x) \, dx \text{ and } \sigma^2 = \int^\infty_{-\infty}(x-\mu)^2f(x) \, dx$$
    where:
    $$ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x - \mu)^2}{2\sigma^2}}$$
\end{quiz}


\section{Lecture 6 Expectation and Variance of special random variable}
\begin{theorem}
    \textbf{Expectation and Variance of C.R.V}
    \begin{enumerate}
        \item For $X \sim Exp(\lambda)$, we have:
        $$ E(X) = \frac{1}{\lambda} \text{ and } Var(X) = \frac{n}{\lambda}$$
        \item For $X \sim Gamma(\alpha, \lambda)$, we have:
        $$ E(X) = \frac{1}{\lambda}\frac{\Gamma(\alpha+1)}{\Gamma(\alpha)}\text{ and } Var(X) = \frac{1}{\lambda^2}\frac{\Gamma(\alpha+2)}{\Gamma(\alpha)} - (\frac{1}{\lambda}\frac{\Gamma(\alpha+1)}{\Gamma(\alpha)})^2$$
        \item For $X \sim N(\mu, \sigma^2)$, we have:
        $$ E(X) = \mu \text{ and } Var(X) = \sigma^2$$
    \end{enumerate}
\end{theorem}

\begin{theorem}
    \textbf{Expectation and Variance of D.R.V}
    \item For $X \sim Bernoulli(p)$, we have:
    $$ E(X) = p \text{ and } Var(X) = p(1-p)$$
    \item For $X \sim Bin(n, p)$, we have:
    $$ E(X) = np \text{ and } Var(X) = np(1-p)$$
    \item For $X \sim Poission(\lambda)$, we have:
    $$ E(X) = \lambda \text{ and } Var(X) = \lambda$$
    \item For $X \sim Geo(p)$, we have:
    $$ E(X) = \frac{1-p}{p} \text{ and } Var(X) =\frac{1-p}{p^2}$$
\end{theorem}

\begin{definition}
    \textbf{Cumulative Distribution Function}\\
    For a random variable $X$, the cumulative distribution function(c.d.f.) of $X$ is:
    $$ F_X(b) = P(X \leq b)$$
\end{definition}
We notice that:
\begin{enumerate}
    \item For discrete random variable:
    $$ F(b) = \Sigma^{[b]}_{m = -\infty}P(X = m) $$
    \item For continuous random variable:
    $$ F'(b) = f(b)$$
\end{enumerate}
But random variable have forms instead of these two kinds. See the quiz below:
\begin{quiz}
    The cumulative distribution function of $X$ is
    \[
    F(x) = 
    \begin{cases}
        0 & x < 0 \\
        \frac{x}{2} & 0 \leq x < 1 \\
        \frac{2}{3} & 1 \leq x < 2 \\
        \frac{11}{12} & 2 \leq x < 3\\
    \end{cases}
    \]
    \begin{enumerate}
        \item[(i)] $P(x < 3)$
        \item[(ii)] $P(x = 1)$
        \item[(iii)] $P(x > \frac{1}{2})$
    \end{enumerate}
\end{quiz}

\begin{theorem}
    \begin{enumerate}
        \item If $A_n \subset A_{n+1}, \forall n \geq 1$, then:
    $$P(\bigcup^{\infty}_{n=1}) = lim_{n \rightarrow \infty}P(A_n)$$
        \item If $B_{n+1} \subset B_{n}, \forall n \geq 1$, then:
    $$P(\bigcap^{\infty}_{n=1}) = lim_{n \rightarrow \infty}P(B_n)$$
    \end{enumerate}
\end{theorem}

\begin{theorem}
    \textbf{Properties of Cumulative Distribution Function:}\\
    Let $F$ be a cumulative distribution function.
    \begin{enumerate}
        \item $F$ is a non-decreasing function, i.e.:
        $$ \forall \text{ } a < b, F(a) \leq F(b)$$
        \item $lim_{b\rightarrow-\infty}F(b) = 0, lim_{b\rightarrow\infty}F(b) = 1$
        \item $F$ is right continuous, i.e.:
        $$\forall \text{ } b \in \mathbb{R}, \forall lim_{n \rightarrow \infty}b_n = b, \text{ we have } lim_{n \rightarrow \infty}F(b_n) = F(b) $$
        \item $F$ has left limits, i.e.:
        $$\forall b \in \mathbb{R}, \forall lim_{n \rightarrow \infty}(a_n) = a, \text{ we have } lim_{n \rightarrow \infty}F(a_n) = F(a^{-}) = F(x < a)$$
    \end{enumerate}
\end{theorem}
Use the \textbf{theorem6.3}, we could easily prove.

For example, we take $X \sim Bernoulli(p)$, then:
\[
F_X(b) = 
\begin{cases}
    1 & b \geq 1 \\
    1-p & 0 \leq b < 1 \\
    0, & b < 0
\end{cases}
\]
It is a very traditional step function.


\section{Lecture 7 Function of Random Varibale 2024.10.17}
\begin{theorem}
    If $X \sim N(\mu, \sigma^2)$, then:
    $$Y = aX+b \sim N(a\mu + b, a^2\sigma^2), a,b \in \mathbb{R}$$
\end{theorem}


\begin{quiz}
    If the pdf of $X$ is:
    $$f(x) = \frac{1}{\pi(1+x^2)}, \quad -\infty < x < \infty$$
    Show that $Y = \frac{1}{X}$ has the same pdf.
\end{quiz}

\begin{theorem}
    Let $X$ be a continuous random variable with pdf $f_X(x)$. Suppose $g(x)$ is a strictly monotonic( increasingly or decreasing ), differentiable function. Then $Y = g(X)$ has a pdf:
    \[
    f_Y(y) = 
    \begin{cases}
        f_X(g^{-1}(y))|\frac{d}{dy}g^{-1}(y)| & \text{ if } y = g(x) \text{ for some } x.\\
        0 & if y \neq g(x), \forall \text{ } x
    \end{cases}
    \]
\end{theorem}
Proof: $\forall y \in \mathbb{R}, F_Y(y) = P(Y \leq y) = P(g(x) \leq y)$. Assume $g$ is increasing. Then $g(X) \leq y \leftrightarrow X \leq g^{-1}(y)$. So, $F_Y(y) = P(X \leq g^{-1}(y)) = F_X(g^{-1}(y))$.


Theorem7.2 isn't useful since it has too many restrictions.

Now we do a summary on how to find a probability density function of $Y = g(X)$
\begin{enumerate}
    \item Find the cdf of $Y = g(X)$, which means do some simple calculation.
    \item Differentiate to find the density.
    \item Specify in what region the result holds.
\end{enumerate}

\begin{theorem}
    Let $F(x)$ be the cdf of any random variable. Define for each $x \in (0,1)$:
    $$ F^{-1}(x) = sup\{y \in \mathbb{R}: F(y) < x\}$$
\end{theorem}






\section{Lecture 8 Multi-variables 2024.10.24}

\begin{definition}
    \textbf{Joint cumulative distribution function:}\\
    For any random variables $X$, $Y$ the joint cumulative distribution function of $X$ and $Y$ is defined by:
    $$ F(a, b) = P(X \leq a, Y \leq b)$$
\end{definition}
Obviously, by the axiom of probability, we should have:
$$ \lim_{a, b \rightarrow \infty}(F(a,b)) = 1 $$
Notice that:
$$ P(X \leq a) = lim_{b \rightarrow \infty}(P(X \leq a, Y \leq b))$$
Denote as $P(X \leq a) = F(a, \infty)$, and this is defined as \textbf{Marginal Probability Density Funcition}.

For discrete multi-random variables, we can define:
\begin{definition}
    \textbf{Joint probability mass function}:
    When $X$, $Y$ are both discrete random variables with p.m.f is given by $p_X,p_Y$.\\
    The joint probability mass function:
    $$ p(X,Y) = P(X = x, Y = y)$$
\end{definition}
Similar to the definition above, we have \textbf{Marginal Probability Mass Function:}
\begin{align*}
    p_X(x) = \Sigma_yP(X = x, Y = y).
\end{align*}



Similar to a single variable, we can also define independence in multi-variables.
\begin{definition}
    \textbf{Independent random variables}\\
    We say $X$, $Y$ are independent if $\forall A,B \in \mathbb{R}$, 
    $$ P(X \in A, Y \in B) = p(X \in A)P(Y \in B)$$
\end{definition}

From the two definitions above, we could induce that:
\begin{theorem}
    Two discrete random variables $X,Y$ are independent if and only if $\forall x,y \in \mathbb{R}$,
    $$ p(x,y) = P_X(x)P_Y(y)$$
\end{theorem}



\begin{definition}
    \textbf{Jointly continuous}\\
    We say $X$ and $Y$ are jointly continuous if there exist a function $f(x,y0$ such that 
    $$\forall \text{ } C \subset \mathbb{R}^2, P((X,Y) \in C) = \int_{(x,y) \in C}f(x,y) \,dxdy$$
\end{definition}
The function \textbf{f(x,y)} is called the\textbf{ joint probability distribution funcition }of $X$ and $Y$.\\

\begin{definition}
    \textbf{ joint cumulative distribution function: }\\
    The joint c.d.f. is then given by:
    $$ f_X(x) = \int^a_{-\infty}\int^b_{-\infty} f(x,y)\, dxdy$$
\end{definition}
The definition of independence is still the same as before.

\begin{definition}
    \textbf{Expectation}\\
    For any joint p.m.f $p(x,y)$ or joint p.d.f $f(x,y)$, we have a $\forall$ function $g : \mathbb{R}\times\mathbb{R} \longrightarrow \mathbb{R}$,
    $$ E(g(X,Y)) = \Sigma_m\Sigma_n(g(m,n)p(m,n))$$
\end{definition}

For example:
$$ g(X,Y) = 1_{X \in A}1_{Y \in B}$$
$$ E[g(X,Y)] = E[1_{X \in A, Y \in B}] = P(X \in A, Y \in B) = \Sigma_m\Sigma_np(m,n)$$
Or, in continuous situations:
$$ E[g(X,Y)] = \int^\infty_{-\infty} \int^\infty_{-\infty}g(X,Y)f(X,Y) \, dxdy$$
We need to notice that $1_{X \in A}$ and $1_{Y \in B}$ are beneficial \textbf{Characteristic functions}.
\bigskip

Another Example: A man and a woman promised to meet at $12:30$P.M.. Assume the time they arrive are $X$ and $Y$ independently and satisfy:
$$ X \sim \text{Unif}(12:15, 12:45) $$
$$ Y \sim \text{Unif}(12:00, 1:00) $$
\begin{enumerate}
    \item Calculate $P(\text{the man arrive first})$
    \begin{equation*}
        X \sim \text{Unif}(-0.5, 0.5)\\
        Y \sim \text{Unif}(-1, 1)
    \end{equation*}
    \begin{align*}
        P(X<Y) &= \int 1_{(X<Y)} \cdot f(x,y) \text{d}x \: \text{d}y\\
               &= \frac{1}{2} \int_{-\frac{1}{2}}^{\frac{1}{2}} \text{d}x \int_{x}^{1} \text{d}y\\
               &= \frac{1}{2} \int_{-\frac{1}{2}}^{\frac{1}{2}} (1-x) \ \text{d}x\\
               &= \frac{1}{2}
    \end{align*}
    \item Find the probability that the first to arrive waits no longer than $5$ minutes.
    \begin{align*}
        P(|X-Y| < \frac{5}{30}) &= \iint 1_{(|X-Y|<\frac{1}{6})} \cdot f(x,y) \ \text{d}x \: \text{d}y\\
                                &= \frac{1}{2} \int_{-\frac{1}{2}}^{\frac{1}{2}} \text{d}x \int_{x-\frac{1}{6}}^{x+\frac{1}{6}} \text{d}y\\
                                &= \frac{1}{6}
    \end{align*}
\end{enumerate}


\begin{quiz}
    The joint probability distribution function: of $X,Y$ is :
    \[
    f(x,y) = 
    \begin{cases}
        c & \text{ if } x^2 + y^2 \leq R^2\\
        0 & \text{ otherwise }
    \end{cases}
    \]
    \begin{enumerate}
        \item[a)] Find $c$
        \item[b)] Find the marginal probability distribution functions of $f_X(x)$ and $f_Y(y)$.
    \end{enumerate}
\end{quiz}
This is the uniform distribution in circle plates.

\begin{definition}
    \textbf{Bivariate Normal Distribution}\\
    The joint probability of bivariate normal distribution is given by:
    $$ f(x,y) = \frac{1}{2\pi \sigma_x\sigma_y\sqrt{1-\rho^2}} \times exp\{-\frac{1}{2(1-\rho^2)^2}[(\frac{x-\mu_x}{\sigma_x})^2 + (\frac{y - \mu_y}{\sigma_y})^2 - 2\rho\frac{(x - \mu_x)(y - \mu_y)}{\sigma_x\sigma_y}\}$$
    We denote it as $(X,Y) \sim N(\mu_x,\mu_y; \sigma^2_x,\sigma^2_y;\rho)$
\end{definition}

\begin{definition}
    \textbf{Covariance}\\
    The covariance of $X,Y$ is:
    $$ Cov(X,Y) := E[(X - EX)(Y - EY)]$$
\end{definition}
By simple calculation, we know that:
$$ Cov(X,X) = E[(X-EX)^2] = Var(X)$$

Now that we have expanded a single variable into bivariable, how can we get higher dimensions?

We could use \textbf{Matrix Form} to gain a beautiful expression of any finite dimension normal distribution.\\
If we let:
\begingroup
\renewcommand{\arraystretch}{1.5}
\[
\begin{array}{lrl}
            & \vec{x} &= (x,y)\\
            & \vec{\mu} &= (\mu_{x}, \mu_{y})\\
            & \Sigma &= 
                    \begin{bmatrix}
                        \sigma_{x}^{2} & \rho \sigma_{x}\sigma_{y} \\
                        \rho \sigma_{x}\sigma_{y} & \sigma_{y}^{2}
                    \end{bmatrix}\\
\Rightarrow & \text{det}(\Sigma) &= \sigma_{x}^{2} \sigma_{y}^{2} (1-\rho^{2})\\
            & \Sigma^{-1} &= \cfrac{1}{1-\rho^{2}} \cdot
                    \begin{bmatrix}
                        \cfrac{1}{\sigma_{x}^{2}} & - \cfrac{\rho}{\sigma_{x} \sigma_{y}}\\
                        - \cfrac{\rho}{\sigma_{x} \sigma_{y}} & \cfrac{1}{\sigma_{y}^{2}}
                    \end{bmatrix}
\end{array}
\]
\endgroup

Then we have:
\[
f(x,y) = \frac{1}{2\pi \sqrt{\text{det}(\Sigma)}} \cdot \text{exp} \left\{ -\frac{1}{2} (\vec{x} - \vec{\mu}) \Sigma^{-1} (\vec{x} - \vec{\mu})^{T}  \right\}
\]
This is a very beautiful structure with general form.

\begin{theorem}
    In Bivariate Normal Distribution, the $X,Y$ independent are equivalent with:
    \begin{enumerate}
        \item $\rho = 0$
        \item $Cov(X,Y) = 0$
    \end{enumerate}
\end{theorem}
\section{Lecture 9 Sum of Independent Random Variables 2024.10.29}
To understand the structure below better, we introduce the convolution of two functions. 
\begin{definition}
    \textbf{Convolution:}
    Let $f$ and $g$ be two functions 
    \begin{equation}
    (f * g)(t) = \int_{-\infty}^{\infty} f(\tau) g(t - \tau) \, d\tau
    \end{equation}
\end{definition}
\begin{theorem}
    \textbf{Sum of independent random variables}\\
    Let $X,Y$ be independent continuous random variables. And $Z = X + Y$, then we have:
    \begin{equation*}
        F_Z(z) = \int^\infty_{-\infty}F_X(z-y)f_Y(y) dy
    \end{equation*}
    Differentiate to obtain:
    \begin{align*}
        f_Z(z) &= \int^\infty_{-\infty}f_X(z-y)f_Y(y) dy \\
               &= (f_X * f_Y)(z)
    \end{align*}
\end{theorem}
Now we compute an example:\\
$X,Y \sim Exp(\lambda)$, and they are independent.\\
Compute the p.d.f. of $X+Y$: \\
\textbf{Solution:}
\begin{align*}
    f_{X+Y}(z)  &= \int^\infty_{-\infty}f_x(z-y)f_Y(y) \, dy \\
                &= \int^z_{0}\lambda e^{\lambda(z-y)} \times \lambda e^{-\lambda y} \, dy \\
                &= \lambda^2 e^{-\lambda z} z
\end{align*}
By observation, we can easily notice that $X + Y \sim Gamma(2, \lambda)$
\begin{quiz}
    If the joint p.d.f. of $(X,Y)$ is :
    \begin{align*}
        f(x,y) = 
        \begin{cases}
            \frac{1}{2}(x+y)e^{-(x+y)}, & 0 < x,y < \infty \\
            0 & \text{ otherwise}
        \end{cases}
    \end{align*}
    Find the p.d.f. of $Z = X+Y$.
\end{quiz}





\section{Lecture 10 Conditional Distribution 2024.11.07}
We begin with some easy conclusions:
\begin{theorem}
Here we discuss the sum of independent discrete random variables.
\begin{enumerate}
    \item \textbf{Poisson: }If $X \sim Poisson(\lambda_1)$ and $Y \sim Poisson(\lambda_2)$ are two independent discrete random variables, then $X+Y \sim Poisson(\lambda_1 + \lambda_2)$
    \item \textbf{Binomial: }If $X \sim Bin(n,p)$ and $Y \sim Bin(m,p)$ are two independent discrete random variables, then $X + Y \sim Bin( n+m, p)$

\end{enumerate}

\end{theorem}
\emph{Proof}:
\begin{enumerate}
    \item For Poisson:
    \begin{align*}
    P(X+Y=n) &\xlongequal{?} \frac{(\lambda_{1}+\lambda_{2})^{n}}{n!} \cdot e^{-(\lambda_{1}+\lambda_{2})}\\
    [\text{Total Probability}] &= \sum_{k=0}^{n} P(X+Y = n |\,X=k) \cdot P(X=k)\\
                               &= \sum_{k=0}^{n} P(Y=n-k) \cdot P(X=k) \\
                               &= \sum_{k=0}^{n} \frac{\lambda_{2}^{n-k}}{(n-k)!} \cdot e^{-\lambda_{2}} \cdot \frac{\lambda_{1}^{n-k}}{k!} \cdot e^{-\lambda_{1}}\\
                               &= e^{-(\lambda_{1} + \lambda_{2})} \cdot \frac{1}{n!} \cdot \sum_{k=0}^{n} \frac{n!}{k!(n-k)!} \lambda_{1}^{k} \lambda_{2}^{n-k}\\
                               &= \frac{(\lambda_{1}+\lambda_{2})^{n}}{n!} \cdot e^{-(\lambda_{1}+\lambda_{2})}
    \end{align*}
    \item For Binomial:
    \begin{align*}
        \forall\, 0 \leq N \leq n+m &\\
        P(X + Y = N) &= \sum^{N \land n}_{k = 0}P(Y = N-k | X = k)P(X = k) \\
                     &= \sum^{N \land n}_{k = 0} \binom{m}{N-k}p^{N-k}(1-p)^{m -(N-k)} \binom{n}{k}p^k(1-p)^{n-k} \\
                     &= \sum^{N \land n}_{k = 0} \binom{m}{N - k}\binom{n}{k}p^{N}(1-p)^{m+n - N}
    \end{align*}
    Since we don't know the specific value of $N$, we need to do a classification discussion.
    \begin{enumerate}
    \item $0\leq N \leq n\wedge m$
          \[\sum_{k=0}^{N} \binom{m}{N-k} \binom{n}{k} p^{N} (1-p)^{m+n-N} = \binom{m+n}{N} p^{N} (1-p)^{m+n-N}\]
    \item $n\vee m \leq N \leq n+m$
          \[\sum_{k=0}^{n} \binom{m}{N-k} \binom{n}{k} p^{N} (1-p)^{m+n-N} = \binom{m+n}{N} p^{N} (1-p)^{m+n-N}\]
    \item $n\wedge m \leq N \leq n\vee m$
          Similar hence omitted.
\end{enumerate}
\end{enumerate}

\begin{definition}
    \textbf{Conditional probability mass function:}\\
    If $X,Y$ are two discrete random variables, we define the conditional probability mass function of $X$ given $Y = y$:
    \begin{align*}
        P_{X|Y}(x|y) &= P(X = x| Y = y) \\
                     &= \frac{p(x,y)}{P_Y(y)}
    \end{align*}    
\end{definition}
\textbf{For example:} \\
Suppose the $p(x,y)$ of $(X,Y)$ is:
\[
\begin{array}{rl}
    p(0,0) = 0.4 & p(0,1) = 0.2\\
    p(1,0) = 0.1 & p(1,1) = 0.3
\end{array}
\]
Find the conditional distribution of $X$ given $Y=1$.

\emph{proof:}
\[
    \begin{array}{c}
        p(Y=1) = p(0,1) + p(1,1) = 0.5\\
        P(X=0|\,Y=1) = \frac{P(X=0,Y=1)}{P(Y=1)} = \frac{p(0,1)}{0.5} = \frac{2}{5}\\
        P(X=1|\,Y=1) = \frac{P(X=1,Y=1)}{P(Y=1)} = \frac{p(1,1)}{0.5} = \frac{3}{5}
    \end{array}
\]
Similarly, we can also define:
\begin{definition}
    \textbf{Conditional cumulative distribution function:}
    If $X,Y$ are two discrete random variables, we define the conditional cumulative distribution function of $X$ given $Y = y$:
    \begin{align*}
        F_{X|Y}(x|y) &= P(X \leq x | Y = y) \\
                     &= \Sigma_{m \leq x} P(X = m| Y = y) \\
                     &= \Sigma_{m \leq x} \frac{P(m, y)}{P(Y = y)}
    \end{align*}        
\end{definition}
\textbf{For example:}\\
If $X$ and $Y$ are independent R.V.s with $X\sim Poisson(\lambda_{1})$, $Y\sim Poisson(\lambda_{2})$,
calculate the conditional distribution of $X$ given $X+Y = n$.

\emph{proof:} \quad $\forall\, 0 \leq k \leq n$:
\begin{align*}
    P(X=k|\,X+Y=n) &= \frac{P(X=k,X+Y=n)}{P(X+Y=n)}\\
                   &= \frac{e^{-\lambda_{1}} \cdot \frac{\lambda_{1}^{k}}{k!} \cdot e^{-\lambda_{2}} \cdot \frac{\lambda_{2}^{n-k}}{(n-k)!}}{e^{-(\lambda_{1}+\lambda_{2})} \cdot \frac{(\lambda_{1}+\lambda_{2})^{n}}{n!}}\\
                   &= \frac{n!}{k!(n-k)!} \cdot \left(\frac{\lambda_{1}}{\lambda_{1}+\lambda_{2}}\right)^{k} \cdot \left(\frac{\lambda_{2}}{\lambda_{1}+\lambda_{2}}\right)^{n-k}\\
                   &= \binom{n}{k} \cdot \left(\frac{\lambda_{1}}{\lambda_{1}+\lambda_{2}}\right)^{k} \cdot \left(1-\frac{\lambda_{1}}{\lambda_{1}+\lambda_{2}}\right)^{n-k}\\
                   &\sim Bin(n,\frac{\lambda_{1}}{\lambda_{1}+\lambda_{2}})
\end{align*}



\begin{quiz}
    Joint probability density function of $(X,Y)$ is:
    \begin{equation*}
        f(x,y) = 
        \begin{cases}
            xe^{-x(y+1)} & 0 < x,y < \infty\\
            0 & \text{ otherwise }
        \end{cases}
    \end{equation*}
    \begin{enumerate}
        \item Find $f_{X|Y}(x|y)$
        \item Find $f_{Y|X}(y|x)$
    \end{enumerate}
\end{quiz}
Now we can see the \textbf{Total Probability Formula} from a new perspective.
\begin{definition}
    \textbf{Total Probability Formula:}\\
    Let $A_1,\dots , A_n$ be mutually exclusive and $S = \bigcup^n_{k=1}A_k$. Then:
    \begin{equation*}
        P(B) = \sum^n_{i=1}P(B|A_i)P(A_i)
    \end{equation*}
    The continuous version:
    \begin{equation*}
        P(B) = \int^\infty_{-\infty}P(B | Y = y)f_Y(y) \, dy
    \end{equation*}
\end{definition}














\section{Lecture 11 Multiple Substitution 2024.11.12}


\begin{definition}
    \textbf{Continuous version of total probability formula}:
    \[ P(B) = \int_{-\infty}^{\infty}P(B|Y=y) \cdot f_{Y}(y) \, dy \]
    \[ \left(f_{Y}(y) = \frac{P(B\cap \{Y=y\})}{P(Y=y)}\right) \]

$ P(B|Y) $ is a P.V. of $Y$.
\[P(B) = E[P(B|Y)] = \int_{-\infty}^{\infty} P(B|Y=y) \cdot f_{Y}(y) \, dy\]

\end{definition}

Now recall the Bivariate Normal Distribution:

\[ f(x,y) = \frac{1}{2\pi \sigma_{x} \sigma_{y} \sqrt{1-\rho^{2}}} \text{exp} \left\{ -\frac{1}{2(1-\rho^{2})} \left[ \left( \frac{x-\mu_{x}}{\sigma_{x}} \right)^{2} + \left( \frac{y-\mu_{y}}{\sigma_{y}} \right)^{2} -2 \rho \frac{(x-\mu_{x})(y-\mu_{y})}{\sigma_{x} \sigma_{y}} \right] \right\} \]

\rule{35em}{0.05ex}

\bigskip

Let:
\[
\left\{
    \begin{aligned}
        X &\sim \mathcal{N} ((\mu_{x},\sigma_{x}^{2}))\\
        Y &\sim \mathcal{N} (\mu_{y}, \sigma_{y}^{2})
    \end{aligned}
\right.
\]

Given $Y = y$, the conditional pdf of $X$ is:

\begin{align*}
    f_{X|Y}(x|y) &= \frac{f(x,y)}{f_{Y}(y)} = \frac{f(x,y)}{\frac{1}{\sqrt{2\pi \sigma_{y}^{2}}} \cdot e^{-\frac{(y-\mu_{y})^{2}}{2\sigma_{y}^{2}}}}\\
                 &\sim N(\mu_{x} + \rho \cdot \frac{\sigma_{x}}{\sigma_{y}}(y-\mu_{y}), \sigma_{x}^{2}(1-\rho^{2})) 
\end{align*}
 If $\rho = 0$, then $X \sim N((\mu_{x},\sigma_{x}^{2})),\  Y \sim N(\mu_{y}, \sigma_{y}^{2})$.
 i.e. They are independent.

\begin{quiz}
    The joint pdf of $(X,Y)$ is:
    \[ f(x,y) = \begin{cases}
        2xe^{x^{2}-y} & 0<x<1,y>x^{2}\\
        0 & \text{otherwise}
    \end{cases} \]
Find $f_{Y|X}(y|x)$ and $P(Y\geq \frac{1}{4} | X=x)$
\end{quiz}

\emph{proof:}

\begin{enumerate}
    \item \[ f_{Y|X}(y|x) = \frac{f(x,y)}{f_{X}(x)} = e^{x^{2} - y} \]
    \item \[ P(Y\geq \frac{1}{4} | X=x) = \int_{x \vee \frac{1}{4}}^{\infty} f_{Y|X}(y|x) dy = \begin{cases}
        e^{x^{2} - \frac{1}{4}} &, 0<x<\frac{1}{4}\\
        1 &, \frac{1}{2} <x<1
    \end{cases}\]
\end{enumerate}

\bigskip
Now we introduce a important method in solving integral problems.

\begin{theorem}
    \textbf{Multiple Substitution}:\\
    Let $U = g(X,Y) \text{ and } V = h(X,Y)$.\\
    The joint pdf of $(U,V) = (g(X,Y),h(X,Y))$ is 
\[ f_{UV}(u,v) = |J(u,v)| \cdot f_{XY}(q(u,v),r(u,v)) \]
\end{theorem}
For example: 

Suppose $(X,Y)$ are independent $N(0,1)$.
\[ f(x,y) = \frac{1}{\sqrt{2 \pi}} \cdot e^{-\frac{x^{2}}{2}} \cdot \, \frac{1}{\sqrt{2 \pi}} \cdot e^{-\frac{y^{2}}{2}} \]

Let 
\[
\left\{
    \begin{aligned}
        X &= R \cdot cos \theta\\
        Y &= R \cdot sin \theta
    \end{aligned}
\right.
\qquad ,(R,\theta) \in \mathbb{R}^{+} \times [0,2\pi)
\]

Consider $(X,Y)$ jointly continuous R.V. with joint pdf $f(x,y)$.\\
Define:
\[
\left\{
    \begin{aligned}
        U &= g(X,Y)\\
        V &= h(X,Y)
    \end{aligned}
\right.
\]

Let $K = \{ (x,y) \in \mathbb{R}^{2}, \ f(x,y) > 0 \}$

Set $G = \{ (g(x,y),h(x,y)) \in \mathbb{R}^{2}, (x,y) \in K \} = \{ (U,V) \}$
\[ K=\mathbb{R}^{2},\ G=[0,\infty) \times [0,2\pi) \] The map from $K$ to $G$ is bijective.

Solve
\[
\left\{
    \begin{aligned}
        u &= g(x,y)\\
        v &= h(x,y)
    \end{aligned}
\right.
\]

To get
\[
\left\{
    \begin{aligned}
        x &= q(u,v)\\
        y &= r(u,v)
    \end{aligned}
\right.
\]


Let 
\[
J(u,v) = \begin{vmatrix}
    x_{u} & x_{v} \\
    y_{u} & y_{v}
   \end{vmatrix}
   ={ \Large \begin{vmatrix}
    \frac{\partial q(u,v)}{\partial u} & \frac{\partial q(u,v)}{\partial v} \\
    \frac{\partial r(u,v)}{\partial u} & \frac{\partial r(u,v)}{\partial v}
   \end{vmatrix}}
\]  

\bigskip

\textbf{Theorem:} The joint pdf of $(U,V) = (g(X,Y),h(X,Y))$ is 
\[ f_{UV}(u,v) = |J(u,v)| \cdot f_{XY}(q(u,v),r(u,v)) \]

Hence we have:
\begin{align*}
    f_{R}(R) &= R \cdot e^{-\frac{R^{2}}{2}}\\
    f_{\Theta}(\Theta) &= \frac{1}{2\pi}
\end{align*}
$\Rightarrow \Theta \sim $Unif(0,$2\pi$) Independent.

\begin{definition}
    \textbf{Box-Muller Algorithm:}\\
    Let $U_{1} , U_{2}$ be independent Unif($0,1$). Note that:
    \[ F_{R}(r) = \int_{0}^{r} R\cdot e^{-\frac{R^{2}}{2}} dR = 1-e^{-\frac{r^{2}}{2}} \]
    
    Then $F_{R}^{-1}(u) = \sqrt{-2\ln(1-u)}$
    
    By letting \[ V_{1} = \sqrt{-2\ln(1-U_{1})} \left( or  = \sqrt{-2\ln(U_{1})}\right) \]
    
    we get: $V_{1} \xlongequal{d} R$
    
    \[ V_{2} = 2\pi U_{2} \sim \text{Unif}(0,2\pi) \Rightarrow V_{2} \xlongequal{d} \Theta \]
    
    Then:\[ (V_{1},V_{2} ) \sim (R,\Theta) \]
    
    So:
    \[
    \left\{
        \begin{aligned}
            X &= V_{1} cos\cdot V_{2}\\
            Y &= V_{1} sin\cdot V_{2} 
        \end{aligned}
    \right.
    \]
    Independent \quad $X,Y \sim \mathcal{N}(0,1) $ 
\end{definition}
This algorithm comes from the idea of solving Gauss Integral and give a clear way of how to transform uniform random variables into normal distributed random variables.



\begin{definition}
    \textbf{General Form of Expectation:}\\
    For a discrete R.V. $X$, the expectation of $X$ is:
    \[ EX = \sum_{m=-\infty}^{\infty} m\cdot P(X=m) \]
    given $E|X| < \infty$.
    \[ E|X| = \sum_{m=-\infty}^{\infty} |m|\cdot P(X=m) <\infty \]
    
    For a continuous R.V. $X$, the expectation is:
    \[ EX = \int_{-\infty}^{\infty} x \cdot f(x) \, dx \]
    
    Given:
    \[ E|X| = \int_{-\infty}^{\infty} |x| \cdot f(x) \, dx < \infty \]
    
\end{definition}
\[ Var(X) = E[(X-EX)^{2}] = \sum_{m=-\infty}^{\infty} (m-EX)^{2} \cdot P(X=m) \]








\section{Lecture 12 Order Statistic 2024.11.14}
\begin{definition}
    \textbf{Order Statistic:}\\
    Let $X_1, X_2\dots, X_n$ be independent identically distributed random variables with a common probability density function $f$ and commutative density function $F$\\
    Then we define an ordered sequence of $X_1,X_2,\dots, X_n$
    \begin{equation*}
        X_{(1)} < X_{(2)} < \dots < X_{(n)}
    \end{equation*}
\end{definition}
\begin{theorem}
    The joint probability density function of $X_{(1)}, X_{(2)}, \dots, X_{(n)}$ is:
    \begin{equation*}
        f_{X_{(1)},X_{(2)},\dots,X_{(n)}}(x_1,x_2,\dots,x_n) = n! \times f(x_1)f(x_2)\dots f(x_n) \times 1_{x_1 < x_2 < \dots < x_n}
    \end{equation*}
\end{theorem}
\emph{Proof:}

Using \underline{\textcolor{purple}{Infitesimal Method}}:
$ \forall \epsilon >0 $ 
small, note 
\begin{align*}
    & P(X_{(1)} \in (x_{1} - \frac{\epsilon}{2}, x_{1} + \frac{\epsilon}{2}, \cdots , X_{(n)} \in (x_{n} - \frac{\epsilon}{2}, x_{n} + \frac{\epsilon}{2}))) \quad \forall x_{1} < x_{2} < \cdots < x_{n}\\
    &= \int_{t_{1} \in (x_{1} - \frac{\epsilon}{2}, x_{1} + \frac{\epsilon}{2})} \dif t_{1} \cdot \int_{t_{2} \in (x_{2} - \frac{\epsilon}{2}, x_{2} + \frac{\epsilon}{2})} \dif t_{2} \cdot \int \cdots \int \dif t_{n}\\
    &\approx f(x_{1},\cdots,x_{n}) \cdot \epsilon^{n} + o(\epsilon^{n}) 
\end{align*}

\bigskip

On the other hand, 
\begin{align*}
    LHS &= n! P(X_{(1)} \in (x_{1} - \frac{\epsilon}{2}, x_{1} + \frac{\epsilon}{2}, \cdots , X_{(n)} \in (x_{n} - \frac{\epsilon}{2}, x_{n} + \frac{\epsilon}{2})))\\
        &= n! \prod_{k=1}^{n}P(X_{k} \in (x_{k} - \frac{\epsilon}{2}, x_{k} \in \frac{\epsilon}{2}))\\
        &\approx n! \prod_{k=1}^{n}[f(x_{k}) \epsilon]
\end{align*}

Hence:
\[ f(x_{1}, x_{2} ,\cdots x_{n}) = n! \prod_{k=1}^{n} f(x_{k})  \]

For example:

If $X_{1}, \cdots ,X_{n}$ are i.i.d. Unif(0,1), then:
\[ f_{X_{(1)},X_{(2)},\cdots,X_{(n)}} (x_{1}, x_{2}, \cdots , x_{n}) = n! \quad 0<x_{1}<x_{2}<\cdots<x_{n}<1 \]

\bigskip

For any $1\leq j \leq n$, the marginal pdf of $X_{(j)}$ is:
\begin{align*}
    f_{X(j)} (x) &= \frac{n!}{(n-j)!(j-1)!} \cdot P(X_{1},\cdots,X_{j-1} <x,X_{j} = x, X_{j+1},\cdots , X_{n} >x)\\
                 &= \frac{n!}{(n-j)!(j-1)!} \cdot x^{j-1} (1-x)^{n-j}
\end{align*}
\[ P(X_{1} < x) = x \quad P(X_{1}>x) = 1-x \quad P(X_{1} = x) \approx 1\]








\section{Lecture 13 2024.11.21}

For example:

$X \sim \mathcal{N}(0,1)$, $E|X|^{\alpha}$


For example:
If $X,Y \sim \mathcal{N}(0,1) $, independent, find $E(|X^{2}+Y^{2}|^{\alpha})$ for $\alpha \in \mathbb{R}$

\emph{Solution:}

\begin{align*}
    E(|X^{2}+Y^{2}|^{\alpha}) &= \iint (x^{2} + y^{2})^{\alpha} \cdot f_{XY}(x,y) \dif x \dif y\\
                              &= \iint (x^{2} + y^{2})^{\alpha} \cdot \frac{1}{2\pi} \cdot e^{-\frac{1}{2} (x^{2} +y^{2})} \dif x \dif y\\
                              &= \int_{0}^{\infty} r \dif r \cdot \int_{0}^{2\pi} \dif \theta \cdot (r^{2})^{\alpha} \cdot \frac{1}{2\pi} \cdot e^{-\frac{1}{2} r^{2}}\\
                              &= \int_{0}^{\infty} r^{2\alpha + 1} \cdot e^{-\frac{1}{2} r^{2}}\dif r \\
                              &\xlongequal{t = \frac{1}{2} r^{2}} \int_{0}^{\infty}e^{-t} \cdot (\sqrt{2\pi})^{2\alpha +1} \cdot \frac{1}{\sqrt{2\pi}}\\
                              &= \begin{cases}
                                2^{\alpha} \cdot \varGamma(\alpha +1) & \forall \alpha > -1\\
                                \infty & \forall \alpha \leq -1
                              \end{cases}
\end{align*}


\newpage

\begin{theorem}
    \textbf{Properties of Expectation:}\\
    Let $X_{1},X_{2},\cdots,X_{n}$ be R.V.s such that $E|X_{i}| < \infty,\,\forall i$.
    \begin{enumerate}
    \item If $c_{0}, c_{1},\cdots , c_{n}\in \mathbb{R}$, then:
          \[ E[c_{0} + c_{1}X_{1} + \cdots c_{n}X_{n}] = c_{0} + c_{1} EX_{1} + \cdots c_{n} EX_{n} \]
    \item If $X_{1}, X_{2}, \cdots X_{n}$ are independent, then $\forall g_{1},g_{2}, \cdots g_{n}$, we have:
          \[ E[\prod_{k=1}^{n}g_{k}(X_{k})] = \prod_{k=1}^{\infty} E[g_{k}(X_{k})] \]
          Remark: $g_{1}(X_{1}), \cdots ,g_{n}(X_{n}) $ are also independent. 
    \end{enumerate} 
\end{theorem}


For example:

A group of $n$ men and $n$ women is lined up at random.
\begin{enumerate}
    \item Find the expectation number of men who have a women next to them.
    
    \emph{Solution:}

    Let:
    \[ X_{i} = \begin{cases}
        1 &, \text{If man i has a woman next to him.}\\
        0 &, \text{Otherwise.}
    \end{cases} \]

    \[ EX_{total} = E[X_{1} + \cdots + X_{n}] = \sum_{k=1}^{n}EX_{k} = nEX_{1} \]
    \[EX_{1} = P(X_{1} = 1) = \frac{1}{2n} \cdot \frac{n}{2n-1} + \frac{1}{2n} \cdot \frac{n}{2n-1}+ (2n - 2) \cdot \frac{1}{2n} \cdot (1- \frac{(n-1)(n-2)}{(2n-1)(2n-2)})\]
    \[ \Rightarrow EX_{1} = \frac{3n-1}{4n-2} \]
    \[\Rightarrow EX_{total} = \frac{n(3n-1)}{4n-2} \]

    \item Repeat part $1^{\circ}$, but assuming that the group is randomly seated at a round table.
    \[ EX_{table} = n \times 2n \times \frac{1}{2n} \cdot (1 - \frac{(n-1)(n-2)}{(2n-1)(2n-2)}) = \frac{3n^{2}}{4n-2} \]
\end{enumerate}


\textbf{For example: }

    $20$ individuals, 10 married couples, 5 tables.
    \begin{enumerate}
    \item If the seating is at random, find the expected number of couples that are seated at the same table.
          \[ EX_{total} = 10EX_{1} = 10 \times \frac{3}{19} = \frac{30}{19}\]
    \item If $2$ men and $2$ women are randomly chosen to be seated at each table, repeat $1^{\circ}$
          \[ EX_{total} = 10EX_{1} = 10 \times \frac{2}{10} = 2\]
    \end{enumerate}


\begin{quiz}
   Suppose there are $n$ people coming to a party, they take turns to sit, for each table it has $p$ probability not sitting it, if there is no table for him to sit, he will start a new table.
   \begin{figure}[H]
       \centering
       \includegraphics[width=0.7\linewidth]{Probability Lecture 13.jpg}
       \caption{Quiz 13.1}
   \end{figure}
   What is the expectation of the numbers of table when all $n$ people have taken their seats.
\end{quiz}



For example, this question has been discussed in lecture 4.\\
(Coupon - Collection problem) Suppose there are $N$ different types of coupons, and each time it is equally likely to be any of the $N$ types.
Find expected number of coupons needed before obtaining a complete set of all the $N$ types.

\emph{Solution:}

Define $X_{i}$, $0 \leq i \leq N-1$ to be the number of additional coupons that needed to obtain after $i$ distinct types have been collected.

\[X_{total\ number} = X_{0} + X_{1} + \cdots + X_{N-1}\]
\[ P(X_{i} = k) = \left( \frac{i}{N} \right)^{k-1} \cdot \frac{N-i}{N} \ \Rightarrow X_{i} \sim Geometric\left(\frac{N-i}{N}\right)\]
\[ EX_{total\ number} = \sum_{i=1}^{N-1}EX_{i} = \sum_{i=0}^{N-1}E\left(Geometric\left(\frac{N-i}{N}\right)\right) = N \cdot \sum_{k=1}^{N} \frac{1}{k} \approx N\cdot ln N\]













\section{Lecture 14 Application of expectation 2024.11.26}

\begin{theorem}
    \begin{equation*}
        P(\bigcup^n_{i=1}E_i) = \sum^n_{k=1}(-1)^{k+1}\sum_{1\leq i_1 < i_2 < \dots < i_k \leq n} P(\bigcap^k_{j=1}E_{i_j})
    \end{equation*}
\end{theorem}

Let:
\begin{align*}
    X_i = 
    \begin{cases}
        1 & \text{if } E_i \text{ occurs} \\
        0 & \text{if not} 
    \end{cases}
\end{align*}
Then it is easy to prove.


The variance of $X$ is given by:
\begin{align*}
    Var(X) = E[(X - EX)^2] = \begin{cases}
        \sum^n_{m = -\infty}(m - \mu)P(X = m) \\
        \int^\infty_{-\infty}(x-\mu)f(x)\,dx
    \end{cases}
\end{align*}
Now we introduce some properties of variance:
\begin{theorem}
    \textbf{Properties of variance:}
    \begin{enumerate}
        \item If $EX = \mu$, Var($X$) $< \infty$, then:
        \begin{enumerate}
            \item Var($a+bX$) = $b^2$Var($X$), $\forall\, a,b \in \mathbb{R}$
            \item Var($X$) = E($(x - \mu)^2$) $<$ E($(x-c)^2$), $\forall \,c \in \mathbb{R} \neq \mu$
        \end{enumerate}
        \item Let $X_1,X_2,\dots,X_n$ be random variables with $Var(X_i) < \infty$, then:
        \begin{enumerate}
            \item Var($\sum^n_{i=1}X_i$) = $\sum^n_{i=1}\sum^n_{j=1}[E(X_iX_j) - \mu_i\mu_j]$
            \item If $X_1,X_2,\dots,X_n$ are independent.
            \begin{equation*}
                Var(\sum^n_{i=1}X_i) = \sum^n_{i=1}Var(X_i)
            \end{equation*}
        \end{enumerate}
    \end{enumerate}
\end{theorem}

\emph{Proof:}
\begin{enumerate}
    \item[1.a] Easy to prove.
    \item[1.b] 
    \begin{align*}
        E[(x-c)^2]  &= E[(x - \mu + \mu -c)^2] \\
                    &= E[(x - \mu)^2] + 2E[(x - \mu)(\mu - c)] + (\mu - c)^2 \\
                    &= E[(x - \mu)^2] + (\mu - c)^2 \\
                    &> E[(x - \mu)^2]
    \end{align*}    
    \item[2.a]
    \begin{align*}
        LHS &= E[(\sum^n_{i=1}X_i - E(\sum^n_{i=1}X_i))^2] \\
            &= E[(\sum^n_{i=1}(X_i - \mu_i))^2] \\
            &= \sum^n_{i=1}\sum^n_{j=1} E(X_iX_j) - \mu_iEX_j - \mu_jEX_i + \mu_i\mu_j \\
            &= \sum^n_{i=1}\sum^n_{j=1}[E(X_iX_j) - \mu_i\mu_j]
    \end{align*}
    \item[2.b] Easy to prove by definition.
\end{enumerate}

\begin{definition}
    \textbf{Corelation:}\\
    Let $X,Y$ be two random variables.
    \begin{equation*}
        \rho = \frac{Cov(X,Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}}
    \end{equation*}
\end{definition}
To prove it, we introduce a well-known theory.
\begin{theorem}
    \textbf{Cauchy-Schwartz Theorem:}\\
    For all random variables $X,Y$:
    \begin{equation*}
        |E(XY)| \leq E(|XY|) \leq \sqrt{EX^2 \times EY^2}
    \end{equation*}
\end{theorem}
\emph{Proof:}

Let $X,Y$ be two random variables. Consider: $E[aX+Y]$ as a function of $a$
\begin{align*}
    E[(aX+Y)^2] &= E[a^2X^2 + 2aXY + Y^2] \\
                &= E[X^2] a^2 + 2E[XY] a + E[Y^2] \\
                &\geq 0 \forall a \in \mathbb{R} \\
            \text{So:}& \\
            \Delta &= (2E[XY])^2 - 4EX^EY^2 \geq 0
\end{align*}

Now we can prove $\rho \in [-1,1]$.

\begin{definition}
    \textbf{Uncorrelated:}\\
    If $rho = 0$, or $Cov(X,Y) = 0$, we say $X$ and $Y$ are uncorrelated.
\end{definition}
It is obvious that if $X, Y$ are independent, then they must be uncorrelated.

\begin{quiz}
    Joint probability density function of $X,Y$ is:
    \begin{equation*}
        f(x,y) = \frac{1}{y} e^{-y-\frac{x}{y}}, x>0,y>0
    \end{equation*}
    Find $EX,EY,Cov(X,Y)$
\end{quiz}

Recall the conditional probability density function of $X$ given $Y = y$ is:
\begin{equation*}
    f_{X|Y} = \frac{f(x,y)}{f_Y(y)}
\end{equation*}
Now we can define:
\begin{definition}
    \textbf{Conditional expectation:}\\
    Let $X$ and $Y$ be two random variables:
    \begin{equation*}
        E(X|Y=y) = \int xf_{X|Y}(x|y) \, dx = \int x \frac{f(x,y)}{f_Y(y)} \, dx
    \end{equation*}
\end{definition}

\begin{theorem}
    \textbf{Total formula of probability:}\\
    If $X,Y$ be two continuous random variables, then:
    \begin{equation*}
        E(X) = \int E(X|Y = y)f_Y(y) \, dy
    \end{equation*}
\end{theorem}













\newpage

\newpage
\appendix


\section{Answer for Quizes}
\begin{enumerate}



\item \textbf{\large Quiz 1}
    \begin{enumerate}
        \item 896
        \item 1000
        \item 910
    \end{enumerate}

\item \textbf{\large Quiz 2}
    \begin{enumerate}
        \item $\frac{\binom{N}{n}}{N^n}$
        \item $\frac{\binom{N+n-1}{n}}{N^n}$
    \end{enumerate}

\item \textbf{\large Quiz 3}
    \begin{enumerate}
        \item $\frac{1}{3}$
        \item $\frac{1}{5}$
        \item 1
    \end{enumerate}

\item \textbf{\large Quiz 4}
    \[
    P(X_n = k) =
    \begin{cases}
        \binom{n}{\frac{n+k}{2}}p^{\frac{n+k}{2}}(1-p)^{\frac{n-k}{2}} & \text{ if } n+k \text{ odd } \\
        0 & \text{ if } n+k \text{ even }
    \end{cases}
    \]


\item \textbf{\large Quiz 5}


\item \textbf{\large Quiz 6}
\begin{enumerate}
    \item[(i)] $\frac{11}{12}$
    \item[(ii)] $\frac{1}{6}$
    \item[(iii)] $\frac{3}{4}$
\end{enumerate}

\item \textbf{\large Quiz 7}


\item \textbf{\large Quiz 8}
\begin{enumerate}
    \item $c \ \frac{1}{\pi R^2}$
    \item $f_X(x) = \frac{2}{\pi R^2}\sqrt{R^2 - x^2}$;  $f_Y(y) = \frac{2}{\pi R^2}\sqrt{R^2 - y^2}$
\end{enumerate}

\item \textbf{\large Quiz 9}
     $f_Z(z) = \frac{1}{2}z^2e^{-z}$



\item \textbf{\large Quiz 10}
\begin{enumerate}
    \item $f_{X|Y}(x|y) = (y+1)^2 xe^{-x(y+1)}$
    \item $f_{Y|X}(y|x) = xe^{-xy}$
\end{enumerate}

\item \textbf{\large Quiz 11}

\item \textbf{\large Quiz 12}

\item \textbf{\large Quiz 13}

\item \textbf{\large Quiz 14}
\begin{enumerate}
    \item $EX = 1$
    \item $EY = 1$
    \item $Cov(X,Y) = 1$
\end{enumerate}
\end{enumerate}


\newpage

\section{Extension Problem}

\newpage

% ------------------------------------------------------------------------------
% Reference and Cited Works
% ------------------------------------------------------------------------------

\bibliographystyle{IEEEtran}

\begin{thebibliography}{1}



\end{thebibliography}
% ------------------------------------------------------------------------------


\end{document}
